# Day 0 — Setup & Data Loading (Prerequisites)

Goal: Set up Databricks and load the **E-Commerce Behavior Data** dataset from Kaggle (`mkechinov/ecommerce-behavior-data-from-multi-category-store`).

---

## ✅ Day 0 Checklist

### Account & Workspace
- [x] Created a Databricks Community Edition account
- [x] Verified email and logged in
- [x] Created and started a cluster (default settings are fine)
- [x] Enabled auto-termination (recommended)

### Kaggle Access
- [x] Logged into Kaggle and generated an API Token (`kaggle.json`)
- [x] Saved my Kaggle `username` and `key`

### Data Download & Storage
- [x] Installed Kaggle CLI inside Databricks
- [x] Configured Kaggle credentials
- [x] Downloaded the dataset zip
- [x] Extracted `2019-Oct.csv` and `2019-Nov.csv`
- [x] Deleted the zip to save space

### Data Validation
- [x] Loaded `2019-Oct.csv` into a Spark DataFrame
- [x] Loaded `2019-Nov.csv` into a Spark DataFrame
- [x] Verified row counts
- [x] Verified schema has **9 columns**
- [x] Displayed a sample of 5 rows

# Day 01 — Platform Setup & First Steps

## Goals
- Understand why Databricks is used vs Pandas/Hadoop
- Learn the basics of Lakehouse architecture
- Get familiar with the Databricks workspace (Workspace, Compute, Catalog/Data Explorer)
- Run first PySpark commands in a notebook

---

## Key Learnings

### Why Databricks vs Pandas / Hadoop?
- **Pandas** is great for small-to-medium datasets that fit in a single machine’s memory; it becomes painful (or impossible) at scale.
- **Hadoop (MapReduce)** scales, but it’s heavy to develop and operate compared to modern engines.
- **Databricks (Apache Spark + Lakehouse)** enables distributed processing, collaborative notebooks, and unified data + AI workflows with better productivity.

### Lakehouse Basics
- A **Lakehouse** combines the flexibility/low-cost storage of a **data lake** with the reliability and performance features of a **data warehouse**.
- With technologies like **Delta Lake**, you get features such as ACID-like reliability, schema enforcement, and better query performance on lake storage.

### Databricks Workspace Structure
- **Workspace**: where notebooks, repos, and dashboards live
- **Compute**: where you choose serverless/cluster resources to run code
- **Catalog / Data Explorer**: where you explore schemas, tables, and data assets

---

## Tasks Completed
- [x] Created Databricks account
- [x] Navigated Workspace, Compute, and Catalog/Data Explorer
- [x] Created first notebook
- [x] Ran basic PySpark commands
